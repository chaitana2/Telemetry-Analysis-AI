{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Telemetry Analysis Tool A cross-platform telemetry analysis tool for race data, optimized for Toyota's CSV format. This tool leverages AI to provide insights into driver performance, predictive lap timing, and anomaly detection. Features Data Ingestion : Robust parsing of Toyota-format CSV telemetry files. Data Preprocessing : Automatic cleaning and conversion of time formats (e.g., MM:SS.sss , +SS.sss ). AI Analysis : Driver Performance Trends : Analyze consistency and pace. Predictive Lap Timing : LSTM-based neural network for forecasting lap times. Anomaly Detection : Isolation Forest model to identify outliers in vehicle performance. Visualization : Interactive dashboard built with PyQt6 and Matplotlib. Cross-Platform : Compatible with Linux, Windows, and macOS. Installation Prerequisites Python 3.8 or higher pip (Python package installer) Setup Clone the repository: bash git clone https://github.com/yourusername/telemetry-analysis-tool.git cd telemetry-analysis-tool Create a virtual environment: bash python3 -m venv venv source venv/bin/activate # Linux/Mac # venv\\Scripts\\activate # Windows Install dependencies: bash pip install -r requirements.txt Usage To launch the application: python src/main.py Importing Data Click the \"Import CSV\" button in the top bar. Select a valid Toyota telemetry CSV file. The data will be loaded into the \"Data View\" tab. Development Running Tests Run the full test suite with coverage: pytest --cov=src tests/ Linting Check code style compliance: flake8 src/ tests/ Documentation Generate and serve documentation locally: mkdocs serve Contributing Please read CONTRIBUTING.md for details on our code of conduct and the process for submitting pull requests. License This project is licensed under the MIT License - see the LICENSE file for details.","title":"Home"},{"location":"#telemetry-analysis-tool","text":"A cross-platform telemetry analysis tool for race data, optimized for Toyota's CSV format. This tool leverages AI to provide insights into driver performance, predictive lap timing, and anomaly detection.","title":"Telemetry Analysis Tool"},{"location":"#features","text":"Data Ingestion : Robust parsing of Toyota-format CSV telemetry files. Data Preprocessing : Automatic cleaning and conversion of time formats (e.g., MM:SS.sss , +SS.sss ). AI Analysis : Driver Performance Trends : Analyze consistency and pace. Predictive Lap Timing : LSTM-based neural network for forecasting lap times. Anomaly Detection : Isolation Forest model to identify outliers in vehicle performance. Visualization : Interactive dashboard built with PyQt6 and Matplotlib. Cross-Platform : Compatible with Linux, Windows, and macOS.","title":"Features"},{"location":"#installation","text":"","title":"Installation"},{"location":"#prerequisites","text":"Python 3.8 or higher pip (Python package installer)","title":"Prerequisites"},{"location":"#setup","text":"Clone the repository: bash git clone https://github.com/yourusername/telemetry-analysis-tool.git cd telemetry-analysis-tool Create a virtual environment: bash python3 -m venv venv source venv/bin/activate # Linux/Mac # venv\\Scripts\\activate # Windows Install dependencies: bash pip install -r requirements.txt","title":"Setup"},{"location":"#usage","text":"To launch the application: python src/main.py","title":"Usage"},{"location":"#importing-data","text":"Click the \"Import CSV\" button in the top bar. Select a valid Toyota telemetry CSV file. The data will be loaded into the \"Data View\" tab.","title":"Importing Data"},{"location":"#development","text":"","title":"Development"},{"location":"#running-tests","text":"Run the full test suite with coverage: pytest --cov=src tests/","title":"Running Tests"},{"location":"#linting","text":"Check code style compliance: flake8 src/ tests/","title":"Linting"},{"location":"#documentation","text":"Generate and serve documentation locally: mkdocs serve","title":"Documentation"},{"location":"#contributing","text":"Please read CONTRIBUTING.md for details on our code of conduct and the process for submitting pull requests.","title":"Contributing"},{"location":"#license","text":"This project is licensed under the MIT License - see the LICENSE file for details.","title":"License"},{"location":"CONTRIBUTING/","text":"Contributing to Telemetry Analysis Tool First off, thanks for taking the time to contribute! Code of Conduct This project adheres to the Contributor Covenant code of conduct. By participating, you are expected to uphold this code. How to Contribute Reporting Bugs Ensure the bug was not already reported. Open a new issue with a clear title and detailed description. Include steps to reproduce the issue. Pull Requests Fork the repo and create your branch from main . If you've added code that should be tested, add tests. Ensure the test suite passes ( pytest ). Make sure your code lints ( flake8 ). Issue that pull request! Coding Style We follow PEP 8 . Use Black for code formatting. Write PEP 257 compliant docstrings for all public modules, classes, and functions. Testing We use pytest for testing. Aim for high code coverage.","title":"Contributing to Telemetry Analysis Tool"},{"location":"CONTRIBUTING/#contributing-to-telemetry-analysis-tool","text":"First off, thanks for taking the time to contribute!","title":"Contributing to Telemetry Analysis Tool"},{"location":"CONTRIBUTING/#code-of-conduct","text":"This project adheres to the Contributor Covenant code of conduct. By participating, you are expected to uphold this code.","title":"Code of Conduct"},{"location":"CONTRIBUTING/#how-to-contribute","text":"","title":"How to Contribute"},{"location":"CONTRIBUTING/#reporting-bugs","text":"Ensure the bug was not already reported. Open a new issue with a clear title and detailed description. Include steps to reproduce the issue.","title":"Reporting Bugs"},{"location":"CONTRIBUTING/#pull-requests","text":"Fork the repo and create your branch from main . If you've added code that should be tested, add tests. Ensure the test suite passes ( pytest ). Make sure your code lints ( flake8 ). Issue that pull request!","title":"Pull Requests"},{"location":"CONTRIBUTING/#coding-style","text":"We follow PEP 8 . Use Black for code formatting. Write PEP 257 compliant docstrings for all public modules, classes, and functions.","title":"Coding Style"},{"location":"CONTRIBUTING/#testing","text":"We use pytest for testing. Aim for high code coverage.","title":"Testing"},{"location":"reference/ai/","text":"AI Module AnomalyDetector Detects anomalies in telemetry data using Isolation Forest. Parameters: contamination ( float , default: 0.05 ) \u2013 The proportion of outliers in the data set. Source code in src/ai/models.py 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 class AnomalyDetector : \"\"\" Detects anomalies in telemetry data using Isolation Forest. Args: contamination (float): The proportion of outliers in the data set. \"\"\" def __init__ ( self , contamination : float = 0.05 ): self . model = IsolationForest ( contamination = contamination , random_state = 42 ) self . scaler = StandardScaler () def train ( self , data : np . ndarray ) -> None : \"\"\" Trains the Isolation Forest model. Args: data (np.ndarray): 2D array of feature data. \"\"\" scaled_data = self . scaler . fit_transform ( data ) self . model . fit ( scaled_data ) def predict ( self , data : np . ndarray ) -> np . ndarray : \"\"\" Predicts if a sample is an outlier. Args: data (np.ndarray): 2D array of feature data. Returns: np.ndarray: Array of predictions (-1 for outlier, 1 for inlier). \"\"\" scaled_data = self . scaler . transform ( data ) return self . model . predict ( scaled_data ) predict ( data ) Predicts if a sample is an outlier. Parameters: data ( ndarray ) \u2013 2D array of feature data. Returns: ndarray \u2013 np.ndarray: Array of predictions (-1 for outlier, 1 for inlier). Source code in src/ai/models.py 58 59 60 61 62 63 64 65 66 67 68 69 def predict ( self , data : np . ndarray ) -> np . ndarray : \"\"\" Predicts if a sample is an outlier. Args: data (np.ndarray): 2D array of feature data. Returns: np.ndarray: Array of predictions (-1 for outlier, 1 for inlier). \"\"\" scaled_data = self . scaler . transform ( data ) return self . model . predict ( scaled_data ) train ( data ) Trains the Isolation Forest model. Parameters: data ( ndarray ) \u2013 2D array of feature data. Source code in src/ai/models.py 48 49 50 51 52 53 54 55 56 def train ( self , data : np . ndarray ) -> None : \"\"\" Trains the Isolation Forest model. Args: data (np.ndarray): 2D array of feature data. \"\"\" scaled_data = self . scaler . fit_transform ( data ) self . model . fit ( scaled_data ) LapTimePredictor Mock LSTM-based neural network for predicting the next lap time. (PyTorch dependency removed due to environment constraints) Parameters: input_size ( int , default: 5 ) \u2013 Number of input features per time step. hidden_size ( int , default: 32 ) \u2013 Number of hidden units in the LSTM layer. Source code in src/ai/models.py 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 class LapTimePredictor : \"\"\" Mock LSTM-based neural network for predicting the next lap time. (PyTorch dependency removed due to environment constraints) Args: input_size (int): Number of input features per time step. hidden_size (int): Number of hidden units in the LSTM layer. \"\"\" def __init__ ( self , input_size : int = 5 , hidden_size : int = 32 ): # super(LapTimePredictor, self).__init__() # self.lstm = nn.LSTM(input_size, hidden_size, batch_first=True) # self.fc = nn.Linear(hidden_size, 1) pass def forward ( self , x : np . ndarray ) -> np . ndarray : \"\"\" Forward pass of the model (Mocked). Args: x (np.ndarray): Input array. Returns: np.ndarray: Predicted lap time (random for demo). \"\"\" # Mock output return np . random . rand ( x . shape [ 0 ], 1 ) forward ( x ) Forward pass of the model (Mocked). Parameters: x ( ndarray ) \u2013 Input array. Returns: ndarray \u2013 np.ndarray: Predicted lap time (random for demo). Source code in src/ai/models.py 24 25 26 27 28 29 30 31 32 33 34 35 def forward ( self , x : np . ndarray ) -> np . ndarray : \"\"\" Forward pass of the model (Mocked). Args: x (np.ndarray): Input array. Returns: np.ndarray: Predicted lap time (random for demo). \"\"\" # Mock output return np . random . rand ( x . shape [ 0 ], 1 ) RaceCoach Provides automated coaching insights based on driver data. Source code in src/ai/models.py 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 class RaceCoach : \"\"\" Provides automated coaching insights based on driver data. \"\"\" def __init__ ( self ): pass def analyze_driver ( self , df : pd . DataFrame , driver_number : int ) -> List [ str ]: \"\"\" Analyzes a specific driver's performance and generates insights. Args: df (pd.DataFrame): The telemetry dataframe. driver_number (int): The car number of the driver to analyze. Returns: List[str]: A list of textual insights. \"\"\" driver_data = df [ df [ 'NUMBER' ] == driver_number ] if driver_data . empty : return [ \"No data for this driver.\" ] insights = [] # Consistency Check if 'FL_TIME_SEC' in driver_data . columns : lap_times = driver_data [ 'FL_TIME_SEC' ] . dropna () else : lap_times = driver_data [ 'TOTAL_TIME_SEC' ] . diff () . dropna () if len ( lap_times ) > 1 : # Changed from > 5 to > 1 for smaller datasets std_dev = lap_times . std () mean_lap = lap_times . mean () insights . append ( f \"Average Lap Time: { mean_lap : .2f } s\" ) insights . append ( f \"Consistency (Std Dev): { std_dev : .2f } s\" ) if std_dev < 0.5 : insights . append ( \"Driver is very consistent.\" ) elif std_dev > 2.0 : insights . append ( \"Driver performance is erratic.\" ) return insights analyze_driver ( df , driver_number ) Analyzes a specific driver's performance and generates insights. Parameters: df ( DataFrame ) \u2013 The telemetry dataframe. driver_number ( int ) \u2013 The car number of the driver to analyze. Returns: List [ str ] \u2013 List[str]: A list of textual insights. Source code in src/ai/models.py 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 def analyze_driver ( self , df : pd . DataFrame , driver_number : int ) -> List [ str ]: \"\"\" Analyzes a specific driver's performance and generates insights. Args: df (pd.DataFrame): The telemetry dataframe. driver_number (int): The car number of the driver to analyze. Returns: List[str]: A list of textual insights. \"\"\" driver_data = df [ df [ 'NUMBER' ] == driver_number ] if driver_data . empty : return [ \"No data for this driver.\" ] insights = [] # Consistency Check if 'FL_TIME_SEC' in driver_data . columns : lap_times = driver_data [ 'FL_TIME_SEC' ] . dropna () else : lap_times = driver_data [ 'TOTAL_TIME_SEC' ] . diff () . dropna () if len ( lap_times ) > 1 : # Changed from > 5 to > 1 for smaller datasets std_dev = lap_times . std () mean_lap = lap_times . mean () insights . append ( f \"Average Lap Time: { mean_lap : .2f } s\" ) insights . append ( f \"Consistency (Std Dev): { std_dev : .2f } s\" ) if std_dev < 0.5 : insights . append ( \"Driver is very consistent.\" ) elif std_dev > 2.0 : insights . append ( \"Driver performance is erratic.\" ) return insights","title":"AI"},{"location":"reference/ai/#ai-module","text":"","title":"AI Module"},{"location":"reference/ai/#src.ai.models.AnomalyDetector","text":"Detects anomalies in telemetry data using Isolation Forest. Parameters: contamination ( float , default: 0.05 ) \u2013 The proportion of outliers in the data set. Source code in src/ai/models.py 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 class AnomalyDetector : \"\"\" Detects anomalies in telemetry data using Isolation Forest. Args: contamination (float): The proportion of outliers in the data set. \"\"\" def __init__ ( self , contamination : float = 0.05 ): self . model = IsolationForest ( contamination = contamination , random_state = 42 ) self . scaler = StandardScaler () def train ( self , data : np . ndarray ) -> None : \"\"\" Trains the Isolation Forest model. Args: data (np.ndarray): 2D array of feature data. \"\"\" scaled_data = self . scaler . fit_transform ( data ) self . model . fit ( scaled_data ) def predict ( self , data : np . ndarray ) -> np . ndarray : \"\"\" Predicts if a sample is an outlier. Args: data (np.ndarray): 2D array of feature data. Returns: np.ndarray: Array of predictions (-1 for outlier, 1 for inlier). \"\"\" scaled_data = self . scaler . transform ( data ) return self . model . predict ( scaled_data )","title":"AnomalyDetector"},{"location":"reference/ai/#src.ai.models.AnomalyDetector.predict","text":"Predicts if a sample is an outlier. Parameters: data ( ndarray ) \u2013 2D array of feature data. Returns: ndarray \u2013 np.ndarray: Array of predictions (-1 for outlier, 1 for inlier). Source code in src/ai/models.py 58 59 60 61 62 63 64 65 66 67 68 69 def predict ( self , data : np . ndarray ) -> np . ndarray : \"\"\" Predicts if a sample is an outlier. Args: data (np.ndarray): 2D array of feature data. Returns: np.ndarray: Array of predictions (-1 for outlier, 1 for inlier). \"\"\" scaled_data = self . scaler . transform ( data ) return self . model . predict ( scaled_data )","title":"predict"},{"location":"reference/ai/#src.ai.models.AnomalyDetector.train","text":"Trains the Isolation Forest model. Parameters: data ( ndarray ) \u2013 2D array of feature data. Source code in src/ai/models.py 48 49 50 51 52 53 54 55 56 def train ( self , data : np . ndarray ) -> None : \"\"\" Trains the Isolation Forest model. Args: data (np.ndarray): 2D array of feature data. \"\"\" scaled_data = self . scaler . fit_transform ( data ) self . model . fit ( scaled_data )","title":"train"},{"location":"reference/ai/#src.ai.models.LapTimePredictor","text":"Mock LSTM-based neural network for predicting the next lap time. (PyTorch dependency removed due to environment constraints) Parameters: input_size ( int , default: 5 ) \u2013 Number of input features per time step. hidden_size ( int , default: 32 ) \u2013 Number of hidden units in the LSTM layer. Source code in src/ai/models.py 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 class LapTimePredictor : \"\"\" Mock LSTM-based neural network for predicting the next lap time. (PyTorch dependency removed due to environment constraints) Args: input_size (int): Number of input features per time step. hidden_size (int): Number of hidden units in the LSTM layer. \"\"\" def __init__ ( self , input_size : int = 5 , hidden_size : int = 32 ): # super(LapTimePredictor, self).__init__() # self.lstm = nn.LSTM(input_size, hidden_size, batch_first=True) # self.fc = nn.Linear(hidden_size, 1) pass def forward ( self , x : np . ndarray ) -> np . ndarray : \"\"\" Forward pass of the model (Mocked). Args: x (np.ndarray): Input array. Returns: np.ndarray: Predicted lap time (random for demo). \"\"\" # Mock output return np . random . rand ( x . shape [ 0 ], 1 )","title":"LapTimePredictor"},{"location":"reference/ai/#src.ai.models.LapTimePredictor.forward","text":"Forward pass of the model (Mocked). Parameters: x ( ndarray ) \u2013 Input array. Returns: ndarray \u2013 np.ndarray: Predicted lap time (random for demo). Source code in src/ai/models.py 24 25 26 27 28 29 30 31 32 33 34 35 def forward ( self , x : np . ndarray ) -> np . ndarray : \"\"\" Forward pass of the model (Mocked). Args: x (np.ndarray): Input array. Returns: np.ndarray: Predicted lap time (random for demo). \"\"\" # Mock output return np . random . rand ( x . shape [ 0 ], 1 )","title":"forward"},{"location":"reference/ai/#src.ai.models.RaceCoach","text":"Provides automated coaching insights based on driver data. Source code in src/ai/models.py 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 class RaceCoach : \"\"\" Provides automated coaching insights based on driver data. \"\"\" def __init__ ( self ): pass def analyze_driver ( self , df : pd . DataFrame , driver_number : int ) -> List [ str ]: \"\"\" Analyzes a specific driver's performance and generates insights. Args: df (pd.DataFrame): The telemetry dataframe. driver_number (int): The car number of the driver to analyze. Returns: List[str]: A list of textual insights. \"\"\" driver_data = df [ df [ 'NUMBER' ] == driver_number ] if driver_data . empty : return [ \"No data for this driver.\" ] insights = [] # Consistency Check if 'FL_TIME_SEC' in driver_data . columns : lap_times = driver_data [ 'FL_TIME_SEC' ] . dropna () else : lap_times = driver_data [ 'TOTAL_TIME_SEC' ] . diff () . dropna () if len ( lap_times ) > 1 : # Changed from > 5 to > 1 for smaller datasets std_dev = lap_times . std () mean_lap = lap_times . mean () insights . append ( f \"Average Lap Time: { mean_lap : .2f } s\" ) insights . append ( f \"Consistency (Std Dev): { std_dev : .2f } s\" ) if std_dev < 0.5 : insights . append ( \"Driver is very consistent.\" ) elif std_dev > 2.0 : insights . append ( \"Driver performance is erratic.\" ) return insights","title":"RaceCoach"},{"location":"reference/ai/#src.ai.models.RaceCoach.analyze_driver","text":"Analyzes a specific driver's performance and generates insights. Parameters: df ( DataFrame ) \u2013 The telemetry dataframe. driver_number ( int ) \u2013 The car number of the driver to analyze. Returns: List [ str ] \u2013 List[str]: A list of textual insights. Source code in src/ai/models.py 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 def analyze_driver ( self , df : pd . DataFrame , driver_number : int ) -> List [ str ]: \"\"\" Analyzes a specific driver's performance and generates insights. Args: df (pd.DataFrame): The telemetry dataframe. driver_number (int): The car number of the driver to analyze. Returns: List[str]: A list of textual insights. \"\"\" driver_data = df [ df [ 'NUMBER' ] == driver_number ] if driver_data . empty : return [ \"No data for this driver.\" ] insights = [] # Consistency Check if 'FL_TIME_SEC' in driver_data . columns : lap_times = driver_data [ 'FL_TIME_SEC' ] . dropna () else : lap_times = driver_data [ 'TOTAL_TIME_SEC' ] . diff () . dropna () if len ( lap_times ) > 1 : # Changed from > 5 to > 1 for smaller datasets std_dev = lap_times . std () mean_lap = lap_times . mean () insights . append ( f \"Average Lap Time: { mean_lap : .2f } s\" ) insights . append ( f \"Consistency (Std Dev): { std_dev : .2f } s\" ) if std_dev < 0.5 : insights . append ( \"Driver is very consistent.\" ) elif std_dev > 2.0 : insights . append ( \"Driver performance is erratic.\" ) return insights","title":"analyze_driver"},{"location":"reference/core/","text":"Core Module DataLoader Handles loading and preprocessing of telemetry data from CSV files. Attributes: raw_data ( Optional [ DataFrame ] ) \u2013 The raw data loaded from the CSV. clean_data ( Optional [ DataFrame ] ) \u2013 The processed and cleaned data. Source code in src/core/data_loader.py 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 class DataLoader : \"\"\" Handles loading and preprocessing of telemetry data from CSV files. Attributes: raw_data (Optional[pd.DataFrame]): The raw data loaded from the CSV. clean_data (Optional[pd.DataFrame]): The processed and cleaned data. \"\"\" def __init__ ( self ): \"\"\"Initialize the DataLoader with empty data attributes.\"\"\" self . raw_data : Optional [ pd . DataFrame ] = None self . clean_data : Optional [ pd . DataFrame ] = None def load_csv ( self , filepath : str ) -> bool : \"\"\" Loads telemetry data from a CSV file. Args: filepath (str): The absolute path to the CSV file. Returns: bool: True if loading was successful, False otherwise. \"\"\" try : self . raw_data = pd . read_csv ( filepath ) print ( f \"Successfully loaded { len ( self . raw_data ) } rows from { filepath } \" ) return True except Exception as e : print ( f \"Error loading CSV: { e } \" ) return False def parse_time_str ( self , time_str : str ) -> float : \"\"\" Converts time strings (MM:SS.sss or +SS.sss) to seconds. Args: time_str (str): The time string to parse. Returns: float: The time in seconds, or np.nan if parsing fails. \"\"\" if pd . isna ( time_str ) or time_str == '' : return np . nan time_str = str ( time_str ) . strip () # Handle \"Gap\" format like \"+1:14.985\" or \"+5.234\" if time_str . startswith ( '+' ): time_str = time_str [ 1 :] try : parts = time_str . split ( ':' ) if len ( parts ) == 3 : # HH:MM:SS.sss hours = float ( parts [ 0 ]) minutes = float ( parts [ 1 ]) seconds = float ( parts [ 2 ]) return hours * 3600 + minutes * 60 + seconds elif len ( parts ) == 2 : # MM:SS.sss minutes = float ( parts [ 0 ]) seconds = float ( parts [ 1 ]) return minutes * 60 + seconds elif len ( parts ) == 1 : # SS.sss return float ( parts [ 0 ]) else : return np . nan except ValueError : return np . nan def preprocess ( self ) -> Optional [ pd . DataFrame ]: \"\"\" Cleans and preprocesses the loaded data. Standardizes column names, converts time columns to seconds, and ensures numeric types for key metrics. Returns: Optional[pd.DataFrame]: The cleaned dataframe, or None if no data is loaded. \"\"\" if self . raw_data is None : print ( \"No data loaded.\" ) return None df = self . raw_data . copy () # Standardize column names (strip whitespace, upper case) df . columns = [ c . strip () . upper () for c in df . columns ] # Convert Time Columns time_cols = [ 'TOTAL_TIME' , 'GAP_FIRST' , 'FL_TIME' , 'DIFF_PREV' ] for col in time_cols : if col in df . columns : df [ f ' { col } _SEC' ] = df [ col ] . apply ( self . parse_time_str ) # Convert Numeric Columns numeric_cols = [ 'POSITION' , 'NUMBER' , 'LAPS' , 'FL_KPH' ] for col in numeric_cols : if col in df . columns : df [ col ] = pd . to_numeric ( df [ col ], errors = 'coerce' ) # Categorical Encoding (Example: STATUS) if 'STATUS' in df . columns : df [ 'STATUS' ] = df [ 'STATUS' ] . astype ( str ) self . clean_data = df return self . clean_data __init__ () Initialize the DataLoader with empty data attributes. Source code in src/core/data_loader.py 14 15 16 17 def __init__ ( self ): \"\"\"Initialize the DataLoader with empty data attributes.\"\"\" self . raw_data : Optional [ pd . DataFrame ] = None self . clean_data : Optional [ pd . DataFrame ] = None load_csv ( filepath ) Loads telemetry data from a CSV file. Parameters: filepath ( str ) \u2013 The absolute path to the CSV file. Returns: bool ( bool ) \u2013 True if loading was successful, False otherwise. Source code in src/core/data_loader.py 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 def load_csv ( self , filepath : str ) -> bool : \"\"\" Loads telemetry data from a CSV file. Args: filepath (str): The absolute path to the CSV file. Returns: bool: True if loading was successful, False otherwise. \"\"\" try : self . raw_data = pd . read_csv ( filepath ) print ( f \"Successfully loaded { len ( self . raw_data ) } rows from { filepath } \" ) return True except Exception as e : print ( f \"Error loading CSV: { e } \" ) return False parse_time_str ( time_str ) Converts time strings (MM:SS.sss or +SS.sss) to seconds. Parameters: time_str ( str ) \u2013 The time string to parse. Returns: float ( float ) \u2013 The time in seconds, or np.nan if parsing fails. Source code in src/core/data_loader.py 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 def parse_time_str ( self , time_str : str ) -> float : \"\"\" Converts time strings (MM:SS.sss or +SS.sss) to seconds. Args: time_str (str): The time string to parse. Returns: float: The time in seconds, or np.nan if parsing fails. \"\"\" if pd . isna ( time_str ) or time_str == '' : return np . nan time_str = str ( time_str ) . strip () # Handle \"Gap\" format like \"+1:14.985\" or \"+5.234\" if time_str . startswith ( '+' ): time_str = time_str [ 1 :] try : parts = time_str . split ( ':' ) if len ( parts ) == 3 : # HH:MM:SS.sss hours = float ( parts [ 0 ]) minutes = float ( parts [ 1 ]) seconds = float ( parts [ 2 ]) return hours * 3600 + minutes * 60 + seconds elif len ( parts ) == 2 : # MM:SS.sss minutes = float ( parts [ 0 ]) seconds = float ( parts [ 1 ]) return minutes * 60 + seconds elif len ( parts ) == 1 : # SS.sss return float ( parts [ 0 ]) else : return np . nan except ValueError : return np . nan preprocess () Cleans and preprocesses the loaded data. Standardizes column names, converts time columns to seconds, and ensures numeric types for key metrics. Returns: Optional [ DataFrame ] \u2013 Optional[pd.DataFrame]: The cleaned dataframe, or None if no data is loaded. Source code in src/core/data_loader.py 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 def preprocess ( self ) -> Optional [ pd . DataFrame ]: \"\"\" Cleans and preprocesses the loaded data. Standardizes column names, converts time columns to seconds, and ensures numeric types for key metrics. Returns: Optional[pd.DataFrame]: The cleaned dataframe, or None if no data is loaded. \"\"\" if self . raw_data is None : print ( \"No data loaded.\" ) return None df = self . raw_data . copy () # Standardize column names (strip whitespace, upper case) df . columns = [ c . strip () . upper () for c in df . columns ] # Convert Time Columns time_cols = [ 'TOTAL_TIME' , 'GAP_FIRST' , 'FL_TIME' , 'DIFF_PREV' ] for col in time_cols : if col in df . columns : df [ f ' { col } _SEC' ] = df [ col ] . apply ( self . parse_time_str ) # Convert Numeric Columns numeric_cols = [ 'POSITION' , 'NUMBER' , 'LAPS' , 'FL_KPH' ] for col in numeric_cols : if col in df . columns : df [ col ] = pd . to_numeric ( df [ col ], errors = 'coerce' ) # Categorical Encoding (Example: STATUS) if 'STATUS' in df . columns : df [ 'STATUS' ] = df [ 'STATUS' ] . astype ( str ) self . clean_data = df return self . clean_data","title":"Core"},{"location":"reference/core/#core-module","text":"","title":"Core Module"},{"location":"reference/core/#src.core.data_loader.DataLoader","text":"Handles loading and preprocessing of telemetry data from CSV files. Attributes: raw_data ( Optional [ DataFrame ] ) \u2013 The raw data loaded from the CSV. clean_data ( Optional [ DataFrame ] ) \u2013 The processed and cleaned data. Source code in src/core/data_loader.py 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 class DataLoader : \"\"\" Handles loading and preprocessing of telemetry data from CSV files. Attributes: raw_data (Optional[pd.DataFrame]): The raw data loaded from the CSV. clean_data (Optional[pd.DataFrame]): The processed and cleaned data. \"\"\" def __init__ ( self ): \"\"\"Initialize the DataLoader with empty data attributes.\"\"\" self . raw_data : Optional [ pd . DataFrame ] = None self . clean_data : Optional [ pd . DataFrame ] = None def load_csv ( self , filepath : str ) -> bool : \"\"\" Loads telemetry data from a CSV file. Args: filepath (str): The absolute path to the CSV file. Returns: bool: True if loading was successful, False otherwise. \"\"\" try : self . raw_data = pd . read_csv ( filepath ) print ( f \"Successfully loaded { len ( self . raw_data ) } rows from { filepath } \" ) return True except Exception as e : print ( f \"Error loading CSV: { e } \" ) return False def parse_time_str ( self , time_str : str ) -> float : \"\"\" Converts time strings (MM:SS.sss or +SS.sss) to seconds. Args: time_str (str): The time string to parse. Returns: float: The time in seconds, or np.nan if parsing fails. \"\"\" if pd . isna ( time_str ) or time_str == '' : return np . nan time_str = str ( time_str ) . strip () # Handle \"Gap\" format like \"+1:14.985\" or \"+5.234\" if time_str . startswith ( '+' ): time_str = time_str [ 1 :] try : parts = time_str . split ( ':' ) if len ( parts ) == 3 : # HH:MM:SS.sss hours = float ( parts [ 0 ]) minutes = float ( parts [ 1 ]) seconds = float ( parts [ 2 ]) return hours * 3600 + minutes * 60 + seconds elif len ( parts ) == 2 : # MM:SS.sss minutes = float ( parts [ 0 ]) seconds = float ( parts [ 1 ]) return minutes * 60 + seconds elif len ( parts ) == 1 : # SS.sss return float ( parts [ 0 ]) else : return np . nan except ValueError : return np . nan def preprocess ( self ) -> Optional [ pd . DataFrame ]: \"\"\" Cleans and preprocesses the loaded data. Standardizes column names, converts time columns to seconds, and ensures numeric types for key metrics. Returns: Optional[pd.DataFrame]: The cleaned dataframe, or None if no data is loaded. \"\"\" if self . raw_data is None : print ( \"No data loaded.\" ) return None df = self . raw_data . copy () # Standardize column names (strip whitespace, upper case) df . columns = [ c . strip () . upper () for c in df . columns ] # Convert Time Columns time_cols = [ 'TOTAL_TIME' , 'GAP_FIRST' , 'FL_TIME' , 'DIFF_PREV' ] for col in time_cols : if col in df . columns : df [ f ' { col } _SEC' ] = df [ col ] . apply ( self . parse_time_str ) # Convert Numeric Columns numeric_cols = [ 'POSITION' , 'NUMBER' , 'LAPS' , 'FL_KPH' ] for col in numeric_cols : if col in df . columns : df [ col ] = pd . to_numeric ( df [ col ], errors = 'coerce' ) # Categorical Encoding (Example: STATUS) if 'STATUS' in df . columns : df [ 'STATUS' ] = df [ 'STATUS' ] . astype ( str ) self . clean_data = df return self . clean_data","title":"DataLoader"},{"location":"reference/core/#src.core.data_loader.DataLoader.__init__","text":"Initialize the DataLoader with empty data attributes. Source code in src/core/data_loader.py 14 15 16 17 def __init__ ( self ): \"\"\"Initialize the DataLoader with empty data attributes.\"\"\" self . raw_data : Optional [ pd . DataFrame ] = None self . clean_data : Optional [ pd . DataFrame ] = None","title":"__init__"},{"location":"reference/core/#src.core.data_loader.DataLoader.load_csv","text":"Loads telemetry data from a CSV file. Parameters: filepath ( str ) \u2013 The absolute path to the CSV file. Returns: bool ( bool ) \u2013 True if loading was successful, False otherwise. Source code in src/core/data_loader.py 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 def load_csv ( self , filepath : str ) -> bool : \"\"\" Loads telemetry data from a CSV file. Args: filepath (str): The absolute path to the CSV file. Returns: bool: True if loading was successful, False otherwise. \"\"\" try : self . raw_data = pd . read_csv ( filepath ) print ( f \"Successfully loaded { len ( self . raw_data ) } rows from { filepath } \" ) return True except Exception as e : print ( f \"Error loading CSV: { e } \" ) return False","title":"load_csv"},{"location":"reference/core/#src.core.data_loader.DataLoader.parse_time_str","text":"Converts time strings (MM:SS.sss or +SS.sss) to seconds. Parameters: time_str ( str ) \u2013 The time string to parse. Returns: float ( float ) \u2013 The time in seconds, or np.nan if parsing fails. Source code in src/core/data_loader.py 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 def parse_time_str ( self , time_str : str ) -> float : \"\"\" Converts time strings (MM:SS.sss or +SS.sss) to seconds. Args: time_str (str): The time string to parse. Returns: float: The time in seconds, or np.nan if parsing fails. \"\"\" if pd . isna ( time_str ) or time_str == '' : return np . nan time_str = str ( time_str ) . strip () # Handle \"Gap\" format like \"+1:14.985\" or \"+5.234\" if time_str . startswith ( '+' ): time_str = time_str [ 1 :] try : parts = time_str . split ( ':' ) if len ( parts ) == 3 : # HH:MM:SS.sss hours = float ( parts [ 0 ]) minutes = float ( parts [ 1 ]) seconds = float ( parts [ 2 ]) return hours * 3600 + minutes * 60 + seconds elif len ( parts ) == 2 : # MM:SS.sss minutes = float ( parts [ 0 ]) seconds = float ( parts [ 1 ]) return minutes * 60 + seconds elif len ( parts ) == 1 : # SS.sss return float ( parts [ 0 ]) else : return np . nan except ValueError : return np . nan","title":"parse_time_str"},{"location":"reference/core/#src.core.data_loader.DataLoader.preprocess","text":"Cleans and preprocesses the loaded data. Standardizes column names, converts time columns to seconds, and ensures numeric types for key metrics. Returns: Optional [ DataFrame ] \u2013 Optional[pd.DataFrame]: The cleaned dataframe, or None if no data is loaded. Source code in src/core/data_loader.py 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 def preprocess ( self ) -> Optional [ pd . DataFrame ]: \"\"\" Cleans and preprocesses the loaded data. Standardizes column names, converts time columns to seconds, and ensures numeric types for key metrics. Returns: Optional[pd.DataFrame]: The cleaned dataframe, or None if no data is loaded. \"\"\" if self . raw_data is None : print ( \"No data loaded.\" ) return None df = self . raw_data . copy () # Standardize column names (strip whitespace, upper case) df . columns = [ c . strip () . upper () for c in df . columns ] # Convert Time Columns time_cols = [ 'TOTAL_TIME' , 'GAP_FIRST' , 'FL_TIME' , 'DIFF_PREV' ] for col in time_cols : if col in df . columns : df [ f ' { col } _SEC' ] = df [ col ] . apply ( self . parse_time_str ) # Convert Numeric Columns numeric_cols = [ 'POSITION' , 'NUMBER' , 'LAPS' , 'FL_KPH' ] for col in numeric_cols : if col in df . columns : df [ col ] = pd . to_numeric ( df [ col ], errors = 'coerce' ) # Categorical Encoding (Example: STATUS) if 'STATUS' in df . columns : df [ 'STATUS' ] = df [ 'STATUS' ] . astype ( str ) self . clean_data = df return self . clean_data","title":"preprocess"},{"location":"reference/ui/","text":"UI Module MainWindow Bases: QMainWindow The main application window for the Telemetry Analysis Tool. Inherits from PyQt6.QtWidgets.QMainWindow. Source code in src/ui/main_window.py 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 class MainWindow ( QMainWindow ): \"\"\" The main application window for the Telemetry Analysis Tool. Inherits from PyQt6.QtWidgets.QMainWindow. \"\"\" def __init__ ( self ): \"\"\"Initialize the main window, set title, size, and load UI.\"\"\" super () . __init__ () self . setWindowTitle ( \"Telemetry Analysis Tool\" ) self . resize ( 1200 , 800 ) self . data_loader = DataLoader () self . data = None self . init_ui () def init_ui ( self ): \"\"\"Sets up the user interface components (layout, buttons, tabs).\"\"\" # Central Widget central_widget = QWidget () self . setCentralWidget ( central_widget ) layout = QVBoxLayout ( central_widget ) # Top Bar (Import Button) top_bar = QHBoxLayout () self . import_btn = QPushButton ( \"Import CSV\" ) self . import_btn . clicked . connect ( self . import_csv ) top_bar . addWidget ( self . import_btn ) top_bar . addStretch () layout . addLayout ( top_bar ) # Tabs self . tabs = QTabWidget () layout . addWidget ( self . tabs ) # Tab 1: Data View self . data_tab = QWidget () self . data_tab_layout = QVBoxLayout ( self . data_tab ) self . data_table = QTableWidget () self . data_tab_layout . addWidget ( self . data_table ) self . tabs . addTab ( self . data_tab , \"Data View\" ) # Tab 2: Dashboard (Placeholder) self . dashboard_tab = QWidget () self . dashboard_layout = QVBoxLayout ( self . dashboard_tab ) self . dashboard_label = QLabel ( \"Dashboard - Visualizations will appear here\" ) self . dashboard_label . setAlignment ( Qt . AlignmentFlag . AlignCenter ) self . dashboard_layout . addWidget ( self . dashboard_label ) self . tabs . addTab ( self . dashboard_tab , \"Dashboard\" ) def import_csv ( self ): \"\"\" Opens a file dialog to select a CSV file and loads it. Displays a success or error message based on the loading result. \"\"\" file_path , _ = QFileDialog . getOpenFileName ( self , \"Open CSV\" , \"\" , \"CSV Files (*.csv)\" ) if file_path : if self . data_loader . load_csv ( file_path ): self . data = self . data_loader . preprocess () self . populate_table () QMessageBox . information ( self , \"Success\" , \"Data loaded successfully!\" ) else : QMessageBox . critical ( self , \"Error\" , \"Failed to load CSV.\" ) def populate_table ( self ): \"\"\" Populates the data table with the loaded dataframe content. \"\"\" if self . data is None : return df = self . data self . data_table . setRowCount ( df . shape [ 0 ]) self . data_table . setColumnCount ( df . shape [ 1 ]) self . data_table . setHorizontalHeaderLabels ( df . columns ) for i in range ( df . shape [ 0 ]): for j in range ( df . shape [ 1 ]): self . data_table . setItem ( i , j , QTableWidgetItem ( str ( df . iloc [ i , j ]))) __init__ () Initialize the main window, set title, size, and load UI. Source code in src/ui/main_window.py 18 19 20 21 22 23 24 25 26 27 def __init__ ( self ): \"\"\"Initialize the main window, set title, size, and load UI.\"\"\" super () . __init__ () self . setWindowTitle ( \"Telemetry Analysis Tool\" ) self . resize ( 1200 , 800 ) self . data_loader = DataLoader () self . data = None self . init_ui () import_csv () Opens a file dialog to select a CSV file and loads it. Displays a success or error message based on the loading result. Source code in src/ui/main_window.py 63 64 65 66 67 68 69 70 71 72 73 74 75 76 def import_csv ( self ): \"\"\" Opens a file dialog to select a CSV file and loads it. Displays a success or error message based on the loading result. \"\"\" file_path , _ = QFileDialog . getOpenFileName ( self , \"Open CSV\" , \"\" , \"CSV Files (*.csv)\" ) if file_path : if self . data_loader . load_csv ( file_path ): self . data = self . data_loader . preprocess () self . populate_table () QMessageBox . information ( self , \"Success\" , \"Data loaded successfully!\" ) else : QMessageBox . critical ( self , \"Error\" , \"Failed to load CSV.\" ) init_ui () Sets up the user interface components (layout, buttons, tabs). Source code in src/ui/main_window.py 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 def init_ui ( self ): \"\"\"Sets up the user interface components (layout, buttons, tabs).\"\"\" # Central Widget central_widget = QWidget () self . setCentralWidget ( central_widget ) layout = QVBoxLayout ( central_widget ) # Top Bar (Import Button) top_bar = QHBoxLayout () self . import_btn = QPushButton ( \"Import CSV\" ) self . import_btn . clicked . connect ( self . import_csv ) top_bar . addWidget ( self . import_btn ) top_bar . addStretch () layout . addLayout ( top_bar ) # Tabs self . tabs = QTabWidget () layout . addWidget ( self . tabs ) # Tab 1: Data View self . data_tab = QWidget () self . data_tab_layout = QVBoxLayout ( self . data_tab ) self . data_table = QTableWidget () self . data_tab_layout . addWidget ( self . data_table ) self . tabs . addTab ( self . data_tab , \"Data View\" ) # Tab 2: Dashboard (Placeholder) self . dashboard_tab = QWidget () self . dashboard_layout = QVBoxLayout ( self . dashboard_tab ) self . dashboard_label = QLabel ( \"Dashboard - Visualizations will appear here\" ) self . dashboard_label . setAlignment ( Qt . AlignmentFlag . AlignCenter ) self . dashboard_layout . addWidget ( self . dashboard_label ) self . tabs . addTab ( self . dashboard_tab , \"Dashboard\" ) populate_table () Populates the data table with the loaded dataframe content. Source code in src/ui/main_window.py 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 def populate_table ( self ): \"\"\" Populates the data table with the loaded dataframe content. \"\"\" if self . data is None : return df = self . data self . data_table . setRowCount ( df . shape [ 0 ]) self . data_table . setColumnCount ( df . shape [ 1 ]) self . data_table . setHorizontalHeaderLabels ( df . columns ) for i in range ( df . shape [ 0 ]): for j in range ( df . shape [ 1 ]): self . data_table . setItem ( i , j , QTableWidgetItem ( str ( df . iloc [ i , j ])))","title":"UI"},{"location":"reference/ui/#ui-module","text":"","title":"UI Module"},{"location":"reference/ui/#src.ui.main_window.MainWindow","text":"Bases: QMainWindow The main application window for the Telemetry Analysis Tool. Inherits from PyQt6.QtWidgets.QMainWindow. Source code in src/ui/main_window.py 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 class MainWindow ( QMainWindow ): \"\"\" The main application window for the Telemetry Analysis Tool. Inherits from PyQt6.QtWidgets.QMainWindow. \"\"\" def __init__ ( self ): \"\"\"Initialize the main window, set title, size, and load UI.\"\"\" super () . __init__ () self . setWindowTitle ( \"Telemetry Analysis Tool\" ) self . resize ( 1200 , 800 ) self . data_loader = DataLoader () self . data = None self . init_ui () def init_ui ( self ): \"\"\"Sets up the user interface components (layout, buttons, tabs).\"\"\" # Central Widget central_widget = QWidget () self . setCentralWidget ( central_widget ) layout = QVBoxLayout ( central_widget ) # Top Bar (Import Button) top_bar = QHBoxLayout () self . import_btn = QPushButton ( \"Import CSV\" ) self . import_btn . clicked . connect ( self . import_csv ) top_bar . addWidget ( self . import_btn ) top_bar . addStretch () layout . addLayout ( top_bar ) # Tabs self . tabs = QTabWidget () layout . addWidget ( self . tabs ) # Tab 1: Data View self . data_tab = QWidget () self . data_tab_layout = QVBoxLayout ( self . data_tab ) self . data_table = QTableWidget () self . data_tab_layout . addWidget ( self . data_table ) self . tabs . addTab ( self . data_tab , \"Data View\" ) # Tab 2: Dashboard (Placeholder) self . dashboard_tab = QWidget () self . dashboard_layout = QVBoxLayout ( self . dashboard_tab ) self . dashboard_label = QLabel ( \"Dashboard - Visualizations will appear here\" ) self . dashboard_label . setAlignment ( Qt . AlignmentFlag . AlignCenter ) self . dashboard_layout . addWidget ( self . dashboard_label ) self . tabs . addTab ( self . dashboard_tab , \"Dashboard\" ) def import_csv ( self ): \"\"\" Opens a file dialog to select a CSV file and loads it. Displays a success or error message based on the loading result. \"\"\" file_path , _ = QFileDialog . getOpenFileName ( self , \"Open CSV\" , \"\" , \"CSV Files (*.csv)\" ) if file_path : if self . data_loader . load_csv ( file_path ): self . data = self . data_loader . preprocess () self . populate_table () QMessageBox . information ( self , \"Success\" , \"Data loaded successfully!\" ) else : QMessageBox . critical ( self , \"Error\" , \"Failed to load CSV.\" ) def populate_table ( self ): \"\"\" Populates the data table with the loaded dataframe content. \"\"\" if self . data is None : return df = self . data self . data_table . setRowCount ( df . shape [ 0 ]) self . data_table . setColumnCount ( df . shape [ 1 ]) self . data_table . setHorizontalHeaderLabels ( df . columns ) for i in range ( df . shape [ 0 ]): for j in range ( df . shape [ 1 ]): self . data_table . setItem ( i , j , QTableWidgetItem ( str ( df . iloc [ i , j ])))","title":"MainWindow"},{"location":"reference/ui/#src.ui.main_window.MainWindow.__init__","text":"Initialize the main window, set title, size, and load UI. Source code in src/ui/main_window.py 18 19 20 21 22 23 24 25 26 27 def __init__ ( self ): \"\"\"Initialize the main window, set title, size, and load UI.\"\"\" super () . __init__ () self . setWindowTitle ( \"Telemetry Analysis Tool\" ) self . resize ( 1200 , 800 ) self . data_loader = DataLoader () self . data = None self . init_ui ()","title":"__init__"},{"location":"reference/ui/#src.ui.main_window.MainWindow.import_csv","text":"Opens a file dialog to select a CSV file and loads it. Displays a success or error message based on the loading result. Source code in src/ui/main_window.py 63 64 65 66 67 68 69 70 71 72 73 74 75 76 def import_csv ( self ): \"\"\" Opens a file dialog to select a CSV file and loads it. Displays a success or error message based on the loading result. \"\"\" file_path , _ = QFileDialog . getOpenFileName ( self , \"Open CSV\" , \"\" , \"CSV Files (*.csv)\" ) if file_path : if self . data_loader . load_csv ( file_path ): self . data = self . data_loader . preprocess () self . populate_table () QMessageBox . information ( self , \"Success\" , \"Data loaded successfully!\" ) else : QMessageBox . critical ( self , \"Error\" , \"Failed to load CSV.\" )","title":"import_csv"},{"location":"reference/ui/#src.ui.main_window.MainWindow.init_ui","text":"Sets up the user interface components (layout, buttons, tabs). Source code in src/ui/main_window.py 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 def init_ui ( self ): \"\"\"Sets up the user interface components (layout, buttons, tabs).\"\"\" # Central Widget central_widget = QWidget () self . setCentralWidget ( central_widget ) layout = QVBoxLayout ( central_widget ) # Top Bar (Import Button) top_bar = QHBoxLayout () self . import_btn = QPushButton ( \"Import CSV\" ) self . import_btn . clicked . connect ( self . import_csv ) top_bar . addWidget ( self . import_btn ) top_bar . addStretch () layout . addLayout ( top_bar ) # Tabs self . tabs = QTabWidget () layout . addWidget ( self . tabs ) # Tab 1: Data View self . data_tab = QWidget () self . data_tab_layout = QVBoxLayout ( self . data_tab ) self . data_table = QTableWidget () self . data_tab_layout . addWidget ( self . data_table ) self . tabs . addTab ( self . data_tab , \"Data View\" ) # Tab 2: Dashboard (Placeholder) self . dashboard_tab = QWidget () self . dashboard_layout = QVBoxLayout ( self . dashboard_tab ) self . dashboard_label = QLabel ( \"Dashboard - Visualizations will appear here\" ) self . dashboard_label . setAlignment ( Qt . AlignmentFlag . AlignCenter ) self . dashboard_layout . addWidget ( self . dashboard_label ) self . tabs . addTab ( self . dashboard_tab , \"Dashboard\" )","title":"init_ui"},{"location":"reference/ui/#src.ui.main_window.MainWindow.populate_table","text":"Populates the data table with the loaded dataframe content. Source code in src/ui/main_window.py 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 def populate_table ( self ): \"\"\" Populates the data table with the loaded dataframe content. \"\"\" if self . data is None : return df = self . data self . data_table . setRowCount ( df . shape [ 0 ]) self . data_table . setColumnCount ( df . shape [ 1 ]) self . data_table . setHorizontalHeaderLabels ( df . columns ) for i in range ( df . shape [ 0 ]): for j in range ( df . shape [ 1 ]): self . data_table . setItem ( i , j , QTableWidgetItem ( str ( df . iloc [ i , j ])))","title":"populate_table"}]}